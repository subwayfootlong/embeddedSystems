# -*- coding: utf-8 -*-
"""MLtraineripynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eV87KBSnaCXqnmeG4ud_Yw3YTVIdOrec
"""

# ============================
# 0. Setup
# ============================
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

print("TF version:", tf.__version__)

# ============================
# 1. Load dataset
# ============================

from google.colab import files
uploaded = files.upload()  # upload sensor_final_labeled_C1_LPG_CO2.csv

csv_name = list(uploaded.keys())[0]
print("Using file:", csv_name)

df = pd.read_csv(csv_name)

# Expecting columns: timestamp, LPG, CO, NH3, CO2, label
print(df.head())
print(df['label'].value_counts())

# ============================
# 2. Prepare features and labels
# ============================

X = df[['LPG', 'CO', 'NH3', 'CO2']].values.astype(np.float32)

label_map = {'normal': 0, 'medium': 1, 'high': 2}
y = df['label'].map(label_map).astype(np.int32).values

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Standardize features (helps training, but you must apply same scaling on Pico)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train).astype(np.float32)
X_test_scaled  = scaler.transform(X_test).astype(np.float32)

num_classes = 3
input_dim = X_train_scaled.shape[1]
input_dim, num_classes

# ============================
# 3. Save scaler params (for Pico)
# ============================

feature_mean = scaler.mean_.astype(np.float32)
feature_std  = np.sqrt(scaler.var_).astype(np.float32)

print("Feature mean:", feature_mean)
print("Feature std:", feature_std)

np.savetxt("safety_scaler_mean.txt", feature_mean)
np.savetxt("safety_scaler_std.txt", feature_std)

files.download("safety_scaler_mean.txt")
files.download("safety_scaler_std.txt")

# ============================
# 4. Build tiny model (8 -> 6 -> 3)
# ============================

from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(8, activation='relu'),
    layers.Dense(6, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ============================
# 5. Train
# ============================

history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_test_scaled, y_test),
    epochs=50,
    batch_size=16,
    verbose=1
)

# ============================
# 6. Evaluate
# ============================

test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)
print("Test accuracy:", test_acc)

# ============================
# 7. Confusion matrix (for report)
# ============================

from sklearn.metrics import confusion_matrix, classification_report

y_pred = model.predict(X_test_scaled)
y_pred_cls = np.argmax(y_pred, axis=1)

print(confusion_matrix(y_test, y_pred_cls))
print(classification_report(y_test, y_pred_cls, target_names=['normal','medium','high']))

# ============================
# 8. Convert to TFLite
# ============================

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # quantization-friendly
tflite_model = converter.convert()

with open("safety_model.tflite", "wb") as f:
    f.write(tflite_model)

print("TFLite size (bytes):", len(tflite_model))
files.download("safety_model.tflite")

# ============================
# 9. Convert TFLite to C array (in Colab shell)
# ============================

!xxd -i safety_model.tflite > safety_model_data.cc
files.download("safety_model_data.cc")

